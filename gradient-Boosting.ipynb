{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Boosting?\n",
    "\n",
    "\n",
    "    Boosting is  many weak predictive model into a strong one, in the form of ensemble of weak models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Gradient Boosting = Gradient descent + Boosting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses :-\n",
    "    * Classification\n",
    "    * Regression\n",
    "    * Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. (source - Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "\n",
    "  Fit an additive model (ensemble) in a forward stage-wise manner.\n",
    " \n",
    "  In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners. In Gradient Boost \"shortcomings\" are identified by gradients while in AdaBoost \"shortcomings\" are identified by high-weight data points. Both tell us how to improve data models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.analyticsvidhya.com/wp-content/uploads/2015/09/boosting.png\" />\n",
    "\n",
    "We start with the first box. We see one vertical line which becomes our first week learner. Now in total we have 3/10 mis-classified observations. We now start giving higher weights to 3 plus mis-classified observations. Now, it becomes very important to classify them right. Hence, the vertical line towards right edge. We repeat this process and then combine each of the learner in appropriate weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## source\n",
    "\n",
    "[pdf](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "[Wikipedia - Gradient Boost](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "\n",
    "[Analytics Vidhya](http://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/)\n",
    "\n",
    "[Tutorial](http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
